<div align="center">

# ğŸ‘“ VisionVoice

## ğŸš€ [**Try VisionVoice Live**](https://visionvoiceai.vercel.app/)

_A modern, multilingual visual aid for the visually impaired, powered by Google Gemini AI._

[![React](https://img.shields.io/badge/React-19.1.1-61DAFB?logo=react&logoColor=white)](https://react.dev/)
[![TypeScript](https://img.shields.io/badge/TypeScript-5.x-3178C6?logo=typescript&logoColor=white)](https://www.typescriptlang.org/)
[![Vite](https://img.shields.io/badge/Vite-6.x-646CFF?logo=vite&logoColor=white)](https://vitejs.dev/)
[![Gemini](https://img.shields.io/badge/AI-Gemini_2.5_Flash-4285F4?logo=google&logoColor=white)](https://deepmind.google/technologies/gemini/)

</div>

---

## ğŸ“– Overview

**VisionVoice** is an accessibility-first web application designed to help visually impaired users interact with their environment. By combining real-time camera processing with Google's state-of-the-art **Gemini 2.5 Flash** model, it provides descriptive narration and text extraction in multiple languages.

---

## âœ¨ Key Features

### ğŸ¯ Dual Analysis Modes
-   **Environmental Description**: Detailed, vivid narrations of scenes, objects, and people to improve spatial awareness.
-   **Text Reading (OCR)**: Accurate extraction and reading of text from documents, labels, signs, and menus.

### ğŸŒ Multilingual Intelligence
-   Supports **8 major languages**: English, à¤¹à¤¿à¤¨à¥à¤¦à¥€ (Hindi), EspaÃ±ol, FranÃ§ais, Deutsch, æ—¥æœ¬èª (Japanese), Italiano, and PortuguÃªs.
-   AI prompts and responses are dynamically localized based on user selection.

### â™¿ Accessibility & UX
-   **Automated Speech (TTS)**: Instant audio playback of AI analysis using the Web Speech API.
-   **Haptic Feedback**: Subtle vibrations provide tactile confirmation for captures, success, and errors.
-   **Mobile-First Design**: Optimized for one-handed use with large tap targets and a "Camera-as-Background" UI.
-   **Hybrid Input**: Use the live camera or upload existing images from your gallery.

---

## ğŸ› ï¸ Architecture & Tech Stack

### Frontend
-   **Framework**: React 19.1.1 with Functional Components.
-   **Logic**: Custom hooks for modular state management (Speech, Haptics, Camera, AI Logic).
-   **Styling**: Tailwind CSS for high-contrast, modern glassmorphism UI.
-   **Icons**: Font Awesome (Solid/Regular).

### AI & Services
-   **Processing**: Google Generative AI (Gemini 2.5 Flash).
-   **Encoding**: Base64 image conversion for serverless AI transmission.

### Code Structure
```text
â”œâ”€â”€ components/
â”‚   â”œâ”€â”€ CameraCapture.tsx    # Live WebRTC video stream & frame capture
â”‚   â”œâ”€â”€ BottomNav.tsx        # Mode switching & primary action triggers
â”‚   â”œâ”€â”€ ResultSheet.tsx      # Slide-up modal for AI output & audio controls
â”‚   â””â”€â”€ Header.tsx           # Language & settings interface
â”œâ”€â”€ hooks/
â”‚   â”œâ”€â”€ useSpeech.ts         # Web Speech API wrapper for announcements & results
â”‚   â”œâ”€â”€ useHaptics.ts        # Tactile feedback via Navigator.vibrate
â”‚   â”œâ”€â”€ useCamera.ts         # Imperative camera handle & capture orchestration
â”‚   â””â”€â”€ useImageProcessing.ts# AI state machine (Loading -> Success -> Error)
â”œâ”€â”€ services/
â”‚   â””â”€â”€ geminiService.ts     # Core API integration with Google GenAI
â”œâ”€â”€ constants.ts             # Multilingual prompt templates & config
â””â”€â”€ types.ts                 # Centralized TypeScript interfaces & Enums
```

---

## ğŸš€ Getting Started

### Prerequisites
-   **Node.js**: v18.0.0 or higher.
-   **API Key**: A valid [Google Gemini API Key](https://aistudio.google.com/).

### Installation

1.  **Clone the Repository**
    ```bash
    git clone https://github.com/vikasmukhiya1999/VisionVoice---Multilingual-Visual-Aid-for-the-Visually-Impaired.git
    cd VisionVoice---Multilingual-Visual-Aid-for-the-Visually-Impaired
    ```

2.  **Install Dependencies**
    ```bash
    npm install
    ```

3.  **Environment Setup**
    Create a `.env.local` file:
    ```env
    GEMINI_API_KEY=your_actual_api_key_here
    ```

4.  **Launch**
    ```bash
    npm run dev
    ```

---

## ï¿½ Internal API Documentation

### Gemini Service (`generateDescription`)
-   **Inputs**: `base64Data`, `mimeType`, `task` (Task Enum), `languageName`.
-   **Logic**: Combines image data with specific prompts:
    -   *Describe*: "Describe this image in detail for a visually impaired person..."
    -   *Read*: "Extract all text from this image, reading it from top to bottom..."

### Haptics Hook (`useHaptics`)
-   Triggers standard browser vibration patterns.
-   **Standard Pattern**: `50ms` pulse for simple confirmations.

---

## ğŸŒ Browser Requirements
-   **Camera Access**: Requires HTTPS or localhost for `navigator.mediaDevices`.
-   **Web Speech API**: Supported on all modern Chrome, Safari, and Edge versions.
-   **Vibration API**: Primarily supported on Android/Mobile Chrome.

---

## ğŸ¤ Contributing
Contributions are welcome! Please ensure:
1.  â™¿ **Accessibility** (ARIA labels, screen reader support) is preserved.
2.  ğŸŒ **Language strings** are accurately translated in `constants.ts`.
3.  ğŸ›¡ï¸ **Error Boundaries** are handled to prevent app crashes on camera failure.

---

<div align="center">

Built with â¤ï¸ for an accessible world.

</div>

## ğŸ“– Code Documentation

### 1. Gemini AI Service (`services/geminiService.ts`)

**Purpose**: Acts as the interface between the client application and Google's Gemini AI API. It handles payload construction and response parsing.

- **Inputs**:
  - `base64Data`: The visual data in base64 format.
  - `mimeType`: Image format (e.g., `image/jpeg`).
  - `task`: Operation mode (`Task.DESCRIBE` or `Task.READ`).
  - `languageName`: Target response language.
- **Outputs**: Returns a `Promise<string>` containing the AI's textual description or extracted text.
- **Example Usage**:
  ```typescript
  const description = await generateDescription(base64, "image/jpeg", Task.DESCRIBE, "Hindi");
  ```

### 2. Image Processing Hook (`hooks/useImageProcessing.ts`)

**Purpose**: Manages the lifecycle of an image analysis request (loading states, errors, and success callbacks).

- **Inputs**: Current language, and callbacks for `onStart`, `onSuccess`, and `onError`.
- **Outputs**: An object containing `image` (preview), `output` (AI result), `isLoading`, and the `processImage` function.
- **Example Usage**:
  ```typescript
  const { processImage, output, isLoading } = useImageProcessing({ 
    language: 'en-US',
    onSuccess: (res) => console.log("AI says:", res)
  });
  ```

### 3. Speech Hook (`hooks/useSpeech.ts`)

**Purpose**: Provides Text-to-Speech (TTS) capabilities for app accessibility.

- **Purpose**: Reads back AI results and provides situational announcements for visually impaired users.
- **Example Usage**:
  ```typescript
  const { speak } = useSpeech({ language: 'en-US' });
  speak("The camera is ready.");
  ```

---

<div align="center">
Built with â¤ï¸ for an accessible world.
</div>
